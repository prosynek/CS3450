{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lab 03 Training a Neural Network Using PyTorch Autograd**\n",
    "\n",
    "Paige Rosynek, 03.30.2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Computing Forward**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 1: no regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = tensor([ 0.0000,  5.2500, 11.5000], grad_fn=<AddBackward0>)\n",
      "h = tensor([ 0.0000,  5.2500, 11.5000], grad_fn=<ReluBackward0>)\n",
      "o = tensor([18.2500, -1.2500], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([0.5, -5.0]) \n",
    "W = torch.tensor([[2.0, 0.5],\n",
    "                 [3.0, 0.25],\n",
    "                 [4.0, 0.1]], requires_grad=True)\n",
    "b1 = torch.tensor([1.5, 5.0, 10.0], requires_grad=True)\n",
    "M = torch.tensor([[5.5, 2.0, 0.5],\n",
    "                 [1.5, 1.0, -1.0]], requires_grad=True)\n",
    "b2 = torch.tensor([2.0, 5.0], requires_grad=True)\n",
    "\n",
    "# input -> hidden\n",
    "z = torch.matmul(W, x) + b1\n",
    "h = torch.relu(z)\n",
    "print(f'z = {z}')\n",
    "print(f'h = {h}')\n",
    "\n",
    "# hidden -> output\n",
    "o = torch.matmul(M, h) + b2\n",
    "print(f'o = {o}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 2: regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = tensor([ 4.0000, -0.5000, 10.0000], grad_fn=<AddBackward0>)\n",
      "h = tensor([ 4.,  0., 10.], grad_fn=<ReluBackward0>)\n",
      "o = tensor([34., 17.], grad_fn=<AddBackward0>)\n",
      "(regularization) s1 = 31.5\n",
      "(regularization) s2 = 31.5\n",
      "regularization s = 3.1500000953674316\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0])   \n",
    "W = torch.tensor([[1.0, 0.5],\n",
    "                 [0.5, -1.0],\n",
    "                 [2.0, 5.0]], requires_grad=True)\n",
    "b1 = torch.tensor([2.0, 1.0, -2.0], requires_grad=True)\n",
    "M = torch.tensor([[5.0, 0.5, 1.0],\n",
    "                 [0.5, 1.0, 2.0]], requires_grad=True)\n",
    "b2 = torch.tensor([4.0, -5.0], requires_grad=True)\n",
    "\n",
    "# input -> hidden\n",
    "z = torch.matmul(W, x) + b1\n",
    "h = torch.relu(z)\n",
    "print(f'z = {z}')\n",
    "print(f'h = {h}')\n",
    "\n",
    "# hidden -> output\n",
    "o = torch.matmul(M, h) + b2\n",
    "print(f'o = {o}')\n",
    "\n",
    "reg = 0.1\n",
    "s1 = torch.sum(W**2) \n",
    "s2 = torch.sum(M**2) \n",
    "s = (reg / 2) * (s1 + s2)\n",
    "print(f'(regularization) s1 = {s1}')\n",
    "print(f'(regularization) s2 = {s2}')\n",
    "print(f'regularization s = {s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Backpropagation with Autograd**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_POINTS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_linear_training_data():\n",
    "    \"\"\"\n",
    "    This method simply rotates points in a 2D space.\n",
    "    Be sure to use MSE in the place of the final softmax layer before testing on this\n",
    "    data!\n",
    "    :return: (x,y) the dataset. x is a numpy array where columns are training samples and\n",
    "             y is a numpy array where columns are one-hot labels for the training sample.\n",
    "    \"\"\"\n",
    "    x = torch.randn((2, TRAINING_POINTS))\n",
    "    x1 = x[0:1, :].clone()\n",
    "    x2 = x[1:2, :]\n",
    "    y = torch.cat((-x2, x1), axis=0)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30          # epochs\n",
    "BATCH_SIZE = 10\n",
    "NUM_BATCHES = TRAINING_POINTS // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Epoch: 1\tLoss: 0.675707221031189\n",
      "-----------------------------------------------\n",
      "Epoch: 2\tLoss: 0.658433735370636\n",
      "-----------------------------------------------\n",
      "Epoch: 3\tLoss: 0.6097180247306824\n",
      "-----------------------------------------------\n",
      "Epoch: 4\tLoss: 0.5260932445526123\n",
      "-----------------------------------------------\n",
      "Epoch: 5\tLoss: 0.44804874062538147\n",
      "-----------------------------------------------\n",
      "Epoch: 6\tLoss: 0.4172821640968323\n",
      "-----------------------------------------------\n",
      "Epoch: 7\tLoss: 0.4070117771625519\n",
      "-----------------------------------------------\n",
      "Epoch: 8\tLoss: 0.3791813552379608\n",
      "-----------------------------------------------\n",
      "Epoch: 9\tLoss: 0.318312406539917\n",
      "-----------------------------------------------\n",
      "Epoch: 10\tLoss: 0.23061519861221313\n",
      "-----------------------------------------------\n",
      "Epoch: 11\tLoss: 0.153119757771492\n",
      "-----------------------------------------------\n",
      "Epoch: 12\tLoss: 0.10446981340646744\n",
      "-----------------------------------------------\n",
      "Epoch: 13\tLoss: 0.07642487436532974\n",
      "-----------------------------------------------\n",
      "Epoch: 14\tLoss: 0.060059383511543274\n",
      "-----------------------------------------------\n",
      "Epoch: 15\tLoss: 0.04995160177350044\n",
      "-----------------------------------------------\n",
      "Epoch: 16\tLoss: 0.042156390845775604\n",
      "-----------------------------------------------\n",
      "Epoch: 17\tLoss: 0.035158671438694\n",
      "-----------------------------------------------\n",
      "Epoch: 18\tLoss: 0.029494144022464752\n",
      "-----------------------------------------------\n",
      "Epoch: 19\tLoss: 0.024690572172403336\n",
      "-----------------------------------------------\n",
      "Epoch: 20\tLoss: 0.02081618271768093\n",
      "-----------------------------------------------\n",
      "Epoch: 21\tLoss: 0.01760844700038433\n",
      "-----------------------------------------------\n",
      "Epoch: 22\tLoss: 0.01503563579171896\n",
      "-----------------------------------------------\n",
      "Epoch: 23\tLoss: 0.013001499697566032\n",
      "-----------------------------------------------\n",
      "Epoch: 24\tLoss: 0.011377297341823578\n",
      "-----------------------------------------------\n",
      "Epoch: 25\tLoss: 0.010088902898132801\n",
      "-----------------------------------------------\n",
      "Epoch: 26\tLoss: 0.008986467495560646\n",
      "-----------------------------------------------\n",
      "Epoch: 27\tLoss: 0.008036104030907154\n",
      "-----------------------------------------------\n",
      "Epoch: 28\tLoss: 0.007212718948721886\n",
      "-----------------------------------------------\n",
      "Epoch: 29\tLoss: 0.006501972675323486\n",
      "-----------------------------------------------\n",
      "Epoch: 30\tLoss: 0.00589129189029336\n",
      "\n",
      "W * M = \n",
      "tensor([[ 2.2021e-03, -9.8698e-01],\n",
      "        [ 1.0062e+00,  3.4809e-04]])\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "reg = 0.001\n",
    "\n",
    "# TODO: Build your network. \n",
    "W = torch.randn(3, 2) * 0.1                  # weights : input -> hidden\n",
    "W.requires_grad = True\n",
    "b_1 = torch.zeros(3, 1, requires_grad=True)     \n",
    "\n",
    "M = torch.randn(2, 3) * 0.1                  # weights : hidden -> output\n",
    "M.requires_grad = True\n",
    "b_2 = torch.zeros(2, 1, requires_grad=True)\n",
    "\n",
    "# TODO: Select your datasource.\n",
    "x_train, y_train = create_linear_training_data()\n",
    "\n",
    "\n",
    "# TODO: Train your network.\n",
    "#---------- training loop ----------\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in range(NUM_BATCHES + 1):\n",
    "        x_batch = x_train[:, i:i+BATCH_SIZE]\n",
    "        y_batch = y_train[:, i:i+BATCH_SIZE]\n",
    "\n",
    "        #---------- forward pass ----------\n",
    "        z = torch.matmul(W, x_batch) + b_1    # input -> hidden\n",
    "        h = torch.relu(z)\n",
    "        o = torch.matmul(M, h) + b_2          # hidden -> output (no activation)\n",
    "        \n",
    "        #---------- regularization ----------\n",
    "        s1 = torch.sum(W**2)                  \n",
    "        s2 = torch.sum(M**2)                  \n",
    "        s = (reg / 2) * (s1 + s2)             # regularization term\n",
    "\n",
    "        # calculate loss - MSE\n",
    "        L = torch.mean((y_batch - o) ** 2)\n",
    "\n",
    "        #---------- objective function ----------\n",
    "        J = L + s \n",
    "\n",
    "        # backpropagation\n",
    "        J.backward()\n",
    "\n",
    "        # update weights - gradient descent\n",
    "        # no_grad : updates weights without changing / tracking the changes in gradients\n",
    "        with torch.no_grad():\n",
    "            W -= learning_rate * W.grad\n",
    "            b_1 -= learning_rate * b_1.grad\n",
    "            M -= learning_rate * M.grad\n",
    "            b_2 -= learning_rate * b_2.grad\n",
    "\n",
    "        # clear the gradients\n",
    "        W.grad.zero_()\n",
    "        b_1.grad.zero_()\n",
    "        M.grad.zero_()\n",
    "        b_2.grad.zero_()\n",
    "\n",
    "    print('-----------------------------------------------')\n",
    "    print(f'Epoch: {epoch + 1}\\tLoss: {J}')\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Sanity-check the output of your network.\n",
    "# You can optionally compute the error on this test data:\n",
    "# x_test, y_test = create_linear_training_data()\n",
    "# h = torch.relu(torch.matmul(W, x_test) + b_1)\n",
    "# y_hat = torch.matmul(M, h) + b_2                 \n",
    "# loss = torch.mean((y_test - y_hat) ** 2)\n",
    "\n",
    "# print('\\n____________________________________')\n",
    "# print(f'Final Loss : {loss}')\n",
    "# print('____________________________________')\n",
    "\n",
    "# But you must computed W*M as discussed in the lab assignment.\n",
    "with torch.no_grad():\n",
    "    print(f'\\nW * M = \\n{torch.matmul(M, W)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
