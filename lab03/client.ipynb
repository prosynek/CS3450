{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# For simple regression problem\n",
    "TRAINING_POINTS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_linear_training_data():\n",
    "    \"\"\"\n",
    "    This method simply rotates points in a 2D space.\n",
    "    Be sure to use MSE in the place of the final softmax layer before testing on this\n",
    "    data!\n",
    "    :return: (x,y) the dataset. x is a numpy array where columns are training samples and\n",
    "             y is a numpy array where columns are one-hot labels for the training sample.\n",
    "    \"\"\"\n",
    "    x = torch.randn((2, TRAINING_POINTS))\n",
    "    x1 = x[0:1, :].clone()\n",
    "    x2 = x[1:2, :]\n",
    "    y = torch.cat((-x2, x1), axis=0)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folded_training_data():\n",
    "    \"\"\"\n",
    "    This method introduces a single non-linear fold into the sort of data created by create_linear_training_data. Be sure to REMOVE the final softmax layer before testing on this data!\n",
    "    Be sure to use MSE in the place of the final softmax layer before testing on this\n",
    "    data!\n",
    "    :return: (x,y) the dataset. x is a numpy array where columns are training samples and\n",
    "             y is a numpy array where columns are one-hot labels for the training sample.\n",
    "    \"\"\"\n",
    "    x = torch.randn((2, TRAINING_POINTS))\n",
    "    x1 = x[0:1, :].copy()\n",
    "    x2 = x[1:2, :]\n",
    "    x2 *= 2 * ((x2 > 0) - 0.5)\n",
    "    y = np.concatenate((-x2, x1), axis=0)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_square():\n",
    "    \"\"\"\n",
    "    This is a square example in which the challenge is to determine\n",
    "    if the points are inside or outside of a point in 2d space.\n",
    "    insideness is true if the points are inside the square.\n",
    "    :return: (points, insideness) the dataset. points is a 2xN array of points and insideness is true if the point is inside the square.\n",
    "    \"\"\"\n",
    "    win_x = [2,2,3,3]\n",
    "    win_y = [1,2,2,1]\n",
    "    win = torch.tensor([win_x,win_y],dtype=torch.float32)\n",
    "    win_rot = torch.cat((win[:,1:],win[:,0:1]),axis=1)\n",
    "    t = win_rot - win # edges tangent along side of poly\n",
    "    rotation = torch.tensor([[0, 1],[-1,0]],dtype=torch.float32)\n",
    "    normal = rotation @ t # normal vectors to each side of poly\n",
    "        # torch.matmul(rotation,t) # Same thing\n",
    "\n",
    "    points = torch.rand((2,2000),dtype = torch.float32)\n",
    "    points = 4*points\n",
    "\n",
    "    vectors = points[:,np.newaxis,:] - win[:,:,np.newaxis] # reshape to fill origin\n",
    "    insideness = (normal[:,:,np.newaxis] * vectors).sum(axis=0)\n",
    "    insideness = insideness.T\n",
    "    insideness = insideness > 0\n",
    "    insideness = insideness.all(axis=1)\n",
    "    return points, insideness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build your network. \n",
    "W_1 = torch.randn(3, 2) * 0.1    # weights : input -> hidden\n",
    "W_1.requires_grad = True\n",
    "b_1 = torch.zeros(3, 1, requires_grad=True)     \n",
    "\n",
    "W_2 = torch.randn(2, 3) * 0.1   # weights : hidden -> output\n",
    "W_2.requires_grad = True\n",
    "b_2 = torch.zeros(2, 1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select your datasource.\n",
    "x_train, y_train = create_linear_training_data()\n",
    "x_test, y_test = create_linear_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # The code in this section should NOT be in a helper method.\n",
    "    # But you may choose to occassionally move helper methods before this as\n",
    "    # the code within them becomes stable.\n",
    "    #\n",
    "    # For this week's lab, however, you can likely keep ALL your code\n",
    "    # right here, with all your variables in the global scope \n",
    "    # for simplified debugging.\n",
    "\n",
    "    # TODO: You may wish to make each TODO below its own pynb cell.\n",
    "    \n",
    "    learning_rate = 0.01\n",
    "    \n",
    "#     # TODO: Build your network. \n",
    "#     W_1 = torch.randn(3, 2, requires_grad=True)       # weights : input -> hidden\n",
    "#     b_1 = torch.zeros(3, 1, requires_grad=True)     \n",
    "    \n",
    "#     W_2 = torch.randn(2, 3, requires_grad=True)     # weights : hidden -> output\n",
    "#     b_2 = torch.zeros(2, 1, requires_grad=True)\n",
    "    \n",
    "\n",
    "    # TODO: Select your datasource.\n",
    "    # x_train, y_train = create_linear_training_data()\n",
    "    \n",
    "\n",
    "    # TODO: Train your network.\n",
    "    for epoch in range(TRAINING_POINTS):\n",
    "        # forward pass\n",
    "        # input -> hidden\n",
    "        z = torch.matmul(W_1, x_train) + b_1\n",
    "        h = torch.relu(z)\n",
    "        s1 = torch.sum(W_1**2)                  # regularization term\n",
    "        \n",
    "        # hidden -> output\n",
    "        y_hat = torch.matmul(W_2, h) + b_2\n",
    "        #y_hat = torch.relu(o)                   # NEED ?\n",
    "\n",
    "        s2 = torch.sum(W_2**2)                  # regularization term\n",
    "        \n",
    "        lambd = 1e-5\n",
    "        s = (lambd / 2) * (s1 + s2)             # final regularization term\n",
    "        \n",
    "        # calculate loss - L2\n",
    "        loss = torch.mean((y_train - y_hat) ** 2)\n",
    "        \n",
    "        regularized_loss = loss + s \n",
    "        \n",
    "        # backpropagation\n",
    "        regularized_loss.backward()\n",
    "        \n",
    "        # update weights - gradient descent\n",
    "        # no_grad : updates weights without changing / tracking the changes in gradients\n",
    "        with torch.no_grad():\n",
    "            W_1 -= learning_rate * W_1.grad\n",
    "            b_1 -= learning_rate * b_1.grad\n",
    "            W_2 -= learning_rate * W_2.grad\n",
    "            b_2 -= learning_rate * b_2.grad\n",
    "\n",
    "        # clear the gradients\n",
    "        W_1.grad.zero_()\n",
    "        b_1.grad.zero_()\n",
    "        W_2.grad.zero_()\n",
    "        b_2.grad.zero_()\n",
    "        \n",
    "        \n",
    "        # print loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print('-----------------------------------------------')\n",
    "            print(f'Epoch: {epoch} : \\tLoss: {regularized_loss}')\n",
    "            \n",
    "            \n",
    "            \n",
    "    # TODO: Sanity-check the output of your network.\n",
    "    # You can optionally compute the error on this test data:\n",
    "    # x_test, y_test = create_linear_training_data()\n",
    "    \n",
    "    # evaluate final model\n",
    "    # input -> hidden\n",
    "    h = torch.relu(torch.matmul(W_1, x_test) + b_1)\n",
    "\n",
    "    # hidden -> output\n",
    "    #y_hat = torch.relu(torch.matmul(W_2, h) + b_2)                   # NEED ReLU ?\n",
    "    y_hat = torch.matmul(W_2, h) + b_2                 \n",
    "\n",
    "    # calculate loss - L2\n",
    "    loss = torch.mean((y_test - y_hat) ** 2)\n",
    "\n",
    "    print('\\n____________________________________')\n",
    "    print(f'Final Loss : {loss}')\n",
    "    print('____________________________________')\n",
    "    \n",
    "    \n",
    "    # But you must computed W*M as discussed in the lab assignment.\n",
    "    # x_test, y_test = create_linear_training_data()\n",
    "\n",
    "    #pass # You may wish to keep this line as a point to place a debugging breakpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
