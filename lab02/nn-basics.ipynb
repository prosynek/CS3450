{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Basics of Feed-Forward Neural Networks\n",
    "\n",
    "**Paige Rosynek**\n",
    "\n",
    "In this lab, we will start to create a feed-forward neural network from scratch.\n",
    "We begin with the very basic computational unit, a perceptron,\n",
    "and then we will add more layers and increase the complexity of our network. Along the way, we will learn how a perceptron works, the benefits of adding more layers, the kind of transformations necessary for learning complex features and relationships from data, and why an object-oriented paradigm is useful for easier management of our neural network framework.\n",
    "\n",
    "We will implement everything from scratch in Python using helpful\n",
    "libraries such as NumPy and PyTorch (without using the autograd feature of PyTorch). The purpose of this lab and the following lab\n",
    "series is to learn how neural networks work starting from the most basic\n",
    "computational units and proceeding to deeper and more networks. This will help us better understand how other popular deep learning frameworks, such as PyTorch, work underneath. You should be able to easily understand and implement everything in this lab. If you are having trouble consult with your instructors as the next lab series will assume a perfect understanding of the basic feed-forward neural network material.\n",
    "\n",
    "The recommended Python version for this implementation is 3.7. Recommended reading: sections 4.1 and 4.2 of the book (https://www.d2l.ai/chapter_multilayer-perceptrons/index.html).\n",
    "\n",
    "## Perceptron\n",
    "\n",
    "A perceptron or artificial neuron is the most basic processing unit of feed-forward neural networks. A perceptron can be modeled as a single-layer neural network with an input vector $\\mathbf{x} \\in \\mathbb{R}^n$, a bias $b$, a vector of trainable weights $\\mathbf{w} \\in \\mathbb{R}^n$, and an output unit $y$. Given the input $\\mathbf{x}$, the output $y$ is computed by an activation function $f(\\cdot)$ as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "y (\\mathbf{x}; \\Theta) = f\\left(\\left(\\sum_{i=1}^{n} x_i w_i\\right) + b \\right) = f(\\mathbf{w}^\\intercal \\mathbf{x} + b)\\,,\n",
    "\\end{equation}\n",
    "where $\\Theta = \\{\\mathbf{w}, b\\}$ represents the trainable parameter set. \n",
    "\n",
    "The figure below shows a schematic view of a single output perceptron. Each input value $x_i$ is multiplied by a weight factor $w_i$. The weighted sum added to the bias is then passed through an activation function to obtain the output, $y$.\n",
    "\n",
    "![MLP example](img/perceptron.png)\n",
    " \n",
    "The vector $\\mathbf{x}$ represents one sample of our data and each element $x_i$ represents a feature. Thus, $\\mathbf{x}$ is often referred to as a feature vector. These features can represent different measurements depending on the application. For example, if we are trying to predict if a patient is at high risk of cardiac disease then each element of $\\mathbf{x}$ might contain vital signs such as diastolic and systolic blood pressure, heart rate, blood sugar levels, etc. In another application where we are trying to predict if a tissue biopsy is cancerous or not using mid-infrared imaging then each element of $\\mathbf{x}$ can represent the amount of mid-infrared light absorbed at a particular wavelength. The output $y$ in the applications above could contain values of $0$ or $1$, indicating if the patient is at high risk of cardiac disease or if the tissue biopsy is cancerous or not.\n",
    " \n",
    "Now, let us begin implementing our first artificial neuron.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Let's assume that our feature vector contains measurements of body temperature pressure, pulse oximeter reading, and presence of cough or not. Then for a 'healthy' patient our input sample might look like $\\mathbf{x} = \\begin{bmatrix} 98.6 \\\\ 95 \\\\ 0 \\end{bmatrix}$. Let's say that we are trying to 'predict' the probability of a patient being positive with COVID-19 based on the above measurements.\n",
    "\n",
    "Each element of our input vector is associated with a unique weight. Let the vector of weights be $\\mathbf{w} = \\begin{bmatrix} 0.03 \\\\ 0.55 \\\\ 0.88 \\end{bmatrix}$. Each artifical neuron is also associated with a unique bias. Let the bias be $b = 2.9$. Assuming a linear activation function write the code to produce and print the output $y$ given the above input vector $\\mathbf{x}$, weights $\\mathbf{w}$, and the bias $b$ using the above perceptron model. Do not use any NumPy or PyTorch functions. Use a Python variable for each element and use Python lists for vectors.\n",
    "\n",
    "For the activation function, use ReLU. This can be computed as ```x * (x > 0)``` in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.108000000000004\n"
     ]
    }
   ],
   "source": [
    "x = [98.6, 95.0, 0.0]\n",
    "w = [0.03, 0.55, 0.88]\n",
    "b = 2.9\n",
    "z = sum(x[i] * w[i] for i in range(3)) + b\n",
    "y = z * (z > 0)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue> Question 1: How many parameters does our simple model contain? Be specific.</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "The model above contains 4 parameters. The number of parameters a model has is calculated by summing the number of weights and biases the model uses. In the case of the model above, the weights vector is 3x1 and there is one bias, b, so the number of parameters can be calculated with the equation: $(3 * 1) + 1 = 4$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue> Question 2: Recall that we were hoping to 'predict' the probability of a patient being positive with COVID-19. Does the output make sense? If not, elaborate on how you could fix it.</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "The output of the neuron above was approximately 58.108. However, this value is not useful for predicting the probability of a patient having COVID-19 and we cannot interpret this as a probability. The reason this value cannot be interpretted as the probability a patient has COVID-19 is because of the activation function we used. The ReLU activation simply makes the output of the neuron 0 if the output of $\\mathbf{w}^\\intercal \\mathbf{x} + b$ is less than zero, otherwise the output of ReLU is the same as the input. Probabilities must be values between 0 and 1, so in order to fix our neuron above to output a probability we need to use a different activation function. The Sigmoid activation function bounds the output of function on the interval $[0, 1]$. Therefore, if we use the Sigmoid function as the activation function of our neuron, the output of the neuron will be a value between 0 and 1 which we can interpret as the probability that a patient has COVID-19, where a value close to 1 indicates that the neuron is confident the patient likely has COVID-19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron with Multiple Outputs\n",
    "\n",
    "The perceptron model above has only one output. However, in most applications, we need multiple outputs. For example, in a classification problem, we would expect the model to output a vector $\\mathbf{y}$, where each $y_i$ represents the probability of a sample belonging to a particular class $i$. The figure below shows a schematic view of a multiple output feed-forward neural network. Each input value $x_i$ is multiplied by a weight factor $W_{ij}$, where $W_{ij}$ denotes a connection weight between the input node $x_i$ and the output node $y_j$. The weighted sum is added to the bias and then passed through an activation function to obtain the output, $y_j$.\n",
    "\n",
    "![Multi outout perceptron](img/multi-output-perceptron.png)\n",
    "\n",
    "Given an input $\\mathbf{x} \\in \\mathbb{R}^n$ this can be modeled as:\n",
    "\n",
    "\\begin{equation}\n",
    "y_j (\\mathbf{x}; \\Theta) = f\\left(\\left(\\sum_{i=1}^{n} x_i W_{ij}\\right) + b_j\\right) = f(\\mathbf{w}_j^\\intercal \\mathbf{x} + b_j)\\,,\n",
    "\\end{equation}\n",
    "where the parameter set here is $\\Theta = \\{ \\mathbf{W} \\in \\mathbb{R}^{n \\times m}, \\mathbf{b} \\in \\mathbb{R}^m \\}$ and $\\mathbf{w}_j$ denotes the $j^{th}$ column of $\\mathbf{W}$. \n",
    "\n",
    "\n",
    "### Implementation\n",
    "\n",
    "\n",
    "Let $\\mathbf{x} = \\begin{bmatrix} 98.6 \\\\ 95 \\\\ 0 \\\\ 1 \\end{bmatrix}$. Let the output vector $\\mathbf{y} \\in \\mathbb{R}^3$, i.e. consisting of $3$ outputs. Let the weights associated with each output node $y_i$ be $\\mathbf{w_1} = \\begin{bmatrix} 0.03 \\\\ 0.55 \\\\ 0.88 \\\\0.73 \\end{bmatrix}$, $\\mathbf{w_2} = \\begin{bmatrix} 0.48 \\\\ 0.31 \\\\ 0.28 \\\\ -0.9 \\end{bmatrix}$, $\\mathbf{w_3} = \\begin{bmatrix} 0.77 \\\\ 0.54 \\\\ 0.32 \\\\ 0.44 \\end{bmatrix}$. Let the bias vector be $\\mathbf{b} = \\begin{bmatrix} 2.9 \\\\ 6.1 \\\\ 3.3 \\end{bmatrix}$. Note that a single bias is associated with each output node $y_i$.\n",
    "\n",
    "Given the above inputs write the code to print the output vector $\\mathbf{y}$.  Use a Python variable for each scalar and use Python lists for vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output vector : y = [1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "w1 = [0.03, 0.55, 0.88, 0.73]\n",
    "w2 = [0.48, 0.31, 0.28, -0.9]\n",
    "w3 = [0.77, 0.54, 0.32, 0.44]\n",
    "\n",
    "x = [98.6, 95, 0, 1]\n",
    "W = [w1, w2, w3]\n",
    "b = [2.9, 6.1, 3.3]\n",
    "\n",
    "y = []\n",
    "for w, b_j in zip(W, b):\n",
    "    z = sum([w_i * x_i for w_i, x_i in zip(w, x)]) + b_j\n",
    "    y.append(sigmoid(z))\n",
    "    \n",
    "print(f'output vector : y = {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "Now that you understand how to do basic computations with a simple perceptron model manually, we will proceed to implement the same model above using matrix-vector operations utilizing PyTorch functions. Organizing the computations in matrix-vector format notation makes it simpler to understand and implement neural network models. \n",
    "\n",
    "Write the code to create the same output vector $\\mathbf{y}$ as above by expressing the above computations as matrix-vector multiplications and summation with a bias vector using code vectorization in PyTorch. You should get the same output as above, up to floating-point errors. Again use a Python variable for each scalar, but use PyTorch arrays for vectors and matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0+cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before sigmoid activation : \n",
      "y = tensor([ 58.8380,  81.9780, 130.9620])\n",
      "\n",
      "output vector : \n",
      "y = tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([98.6, 95, 0, 1])\n",
    "b = torch.tensor([2.9, 6.1, 3.3])\n",
    "W = torch.tensor([[0.03, 0.55, 0.88, 0.73],\n",
    "                  [0.48, 0.31, 0.28, -0.9],\n",
    "                  [0.77, 0.54, 0.32, 0.44]])\n",
    "\n",
    "y = torch.sigmoid((torch.matmul(W, x) + b))\n",
    "\n",
    "print(f'before sigmoid activation : \\ny = {(torch.matmul(W, x) + b)}\\n')\n",
    "print(f'output vector : \\ny = {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([4])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(W.shape)\n",
    "print(x.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "### <font color=blue>Question 3: Explain what each of the dimensions of the matrix of weights $\\mathbf{W}$ and the vector of biases $\\mathbf{b}$ represent?</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "\n",
    "The weight matrix, $W$, has the shape $3x4$. The number of rows corresponds to the number of neurons in the output of the layer, which is 3. The number of columns of the weight matrix represents a weight for each feature, the number of features is 4, so there are 4 weights or columns. Each row of the weights matrix represents a set of weights that are used to calculate each of the output neurons. Therefore, the $i^{th}$ row of the weight matrix, $W$, represents the set of weights to calculate the $i^{th}$ output neuron. The bias vector, $b$, is the shape $3x1$. This corresponds to the shape of the output vector, $y$, which also has the shape $3x1$, because there is a single bias for each output neuron. \n",
    "\n",
    "Note: I understand that a 1 dimensional tensor should be a row vector, so the weight matrix *should* be transposed in the code above. However, when I attempted this, the shapes did not match up and I found the one dimensional tensors (x and b) acted as a column vector which explains my implementation above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Question 4: What is the total number of parameters for this model?</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "\n",
    "The total number of parameters for this model can be calculated by summing the number of weights and biases the model has. The shape of the weights matrix is $3x4$ and the shape of the biases vector is $3x1$. Therefore the total number of parameters can be calculated using the equation: $(3*4) + (3*1) = 15$ model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "## More Layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "A single-layer perceptron network still represents a linear classifier, even if we were to use nonlinear activation functions. This limitation can be overcome by multi-layer neural networks in combination with nonlinear activation functions, which introduce one or more 'hidden' layers between the input and output layers. Multi-layer neural networks are composed of several simple artificial neurons such that the output of one acts as the input of another. A multi-layer neural network can be represented by a composition function. For a two-layer network with only one output, the composition function can be written as\n",
    "\n",
    "\\begin{equation}\n",
    "y_j (\\mathbf{x}; \\Theta) = f^{(2)}\\left(\\sum_{k=1}^{h}W_{kj}^{(2)}*f^{(1)}\\left(\\left(\\sum_{i=1}^{n}W_{ik}^{(1)}*x_i \\right)+b_k^{(1)}\\right)+b_j^{(2)}\\right)\n",
    "\\end{equation}\n",
    "where $h$ is the number of units in the hidden layer and the set of unknown parameters is $\\Theta = \\{\\mathbf{W}^{(1)} \\in R^{n \\times h}, \\mathbf{W}^{(2)} \\in R^{h \\times 1}\\}$. In general, for $L - 1$ hidden layers the composition function, omitting the bias terms, can be written as\n",
    "\\begin{equation}\n",
    "y_j (\\mathbf{x}; \\Theta) = f^{(L)}\\left(\\sum_k W_{kj}^{L}*f^{L-1}\\left(\\sum_{l}W_{lk}^{L - 1}* f^{L - 2}\\left( \\cdots f^{1}\\left(\\sum_{i}W_{iz}^{1}*x_i \\right)\\right) \\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "The figure below illustrates a feed-forward neural network composed of an input layer, a hidden layer, and an output layer. In this illustration, the multi-layer neural network has one input layer and one output unit. In most models, the number of hidden layers and output units is more than one.\n",
    "\n",
    "![Feed forward perceptron](img/feed-forward.png)\n",
    "\n",
    "We will now see how to add an additional layer to our model and then how to generalize to any number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add another layer we need another set of weights and biases.\n",
    "W_2 = torch.Tensor([[-0.3, 0.66, 0.98],\n",
    "                    [0.58, -0.4, 0.38],\n",
    "                    [0.87, 0.69, -0.4]])\n",
    "\n",
    "b_2 = torch.Tensor([3.9, 8.2, 0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "origin_pos": 28
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before sigmoid activation : \n",
      "y2 = tensor([5.2400, 8.7600, 1.9600])\n",
      "\n",
      "output vector : \n",
      "y2 = tensor([0.9947, 0.9998, 0.8765])\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write the code to print the output of a 2-layer feed-forward network using the previously computed output,\n",
    "# y, as input to the second layer \n",
    "y_2 = torch.sigmoid((torch.matmul(W_2, y) + b_2))\n",
    "\n",
    "print(f'before sigmoid activation : \\ny2 = {(torch.matmul(W_2, y) + b_2)}\\n')\n",
    "print(f'output vector : \\ny2 = {y_2}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 30,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "### <font color=blue>Question 5: Explain the dimensions of the weight matrix for the second layer with respect to the dimensions of the previous layer and the number of artificial neurons in the second layer. or Why are the dimensions of the weight matrix of the second layer 3x3?</font>\n",
    "\n",
    "Ans:\n",
    "\n",
    "The shape of the first hidden layer, which is also the input to the second hidden layer, is $3 \\times 1$. There are 3 neurons in the output layer, and 3 neurons in the second hidden layer, therefore, the shape of the weight matrix, $W_2$, must be $3 \\times 3$. If we represent the weight matrix between the first and second hidden layer as $W_2$, and represent the first hidden layer as $y_1$. We know that $y_1 \\in R^{3 \\times 1}$ since the first hidden layer has 3 neurons and we also know $y_2 \\in R^{3 \\times 1}$, so the third hidden layer has 3 neurons. Therefore, for the matrix multiplication, $W_2*y_1$, to be true $W_2$ must have the same number of columns as $y_1$ has rows, and the resulting shape of this operation must be $3 \\times 1$, so $3 \\times 3$ must be the shape of the weight matrix. Additionally, we can infer the size of the weight matrix between the first and second hidden layer using what we know about the size of these two layers. We know that the first hidden layer, which is the input to the second hidden layer, has 3 neurons and we know that the second hidden layer (the output layer) has 3 neurons. The number of rows of a weight matrix corresponds to the number of neurons in the output layer and the number of columns of a weight matrix corresponds to the number of nuerons of the input layer. Since both the input layer (first hidden layer) and the output layer (second hidden layer) have 3 neurons, the weight matrix between these layers should have a shape $3 \\times 3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers to Objects\n",
    "\n",
    "Now, we have a feed-forward model (with an input layer, one hidden layer, and one output layer with 3 outputs) capable of processing a batch of data. It would be cumbersome and redundant if we had to keep writing the same code for hundreds of layers. So, to make our code more modular, easier to manage, and less redundant we will represent layers using an object-oriented programming paradigm. Let's define classes for representing our layers.\n",
    "\n",
    "All layer objects should have an `output` instance attribute.  Use good object-oriented practices to avoid code duplication.  To initialize an instance attribute in Python, write `self.attribute_name = attribute_value` in the initializer (`__init__` method).  Don't mention the variable at the top of the class as we would usually do in Java -- this is how you define static attributes in Python.\n",
    "\n",
    "Rather than each layer taking PyTorch arrays as inputs, it should take `Layer`s as inputs, with each layer having its own name. For example, if your network would take $\\mathbf{x}$, $\\mathbf{W}$, and $\\mathbf{b}$ as inputs, you should have attributes `self.x`, `self.W`, and `self.b`.  Then, when you need the values of these inputs, go back and read the output of the previous layer.  For example, if your layer needs the value of $\\mathbf{W}$, you could read `self.W.output` to get it.\n",
    "\n",
    "Two more Python OO hints: (1) `class MyClass1(MyClass2)` is not a constructor call. It is specifying the inheritance relationship. The Java equivalent is `class MyClass1 extends MyClass2`. So you don't want to add arguments on this line. An easy mistake to make.  (2) You must use `self.` every time you access an instance variable in Python. This is how the language was designed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the following classes.\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, output_shape):\n",
    "        \"\"\"\n",
    "        TODO: Initialize instance attributes here.\n",
    "        \n",
    "        :param output_shape (tuple): the shape of the output array.  When this isa single number, it gives the number of output neurons\n",
    "            When this is an array, it gives the dimensions of the array of output neurons.\n",
    "        \"\"\"\n",
    "        self.output_shape = output_shape\n",
    "        self.output = torch.zeros(output_shape)\n",
    "\n",
    "class Input(Layer):\n",
    "    def __init__(self, output_shape):\n",
    "        \"\"\"\n",
    "        TODO: Accept any arguments specific to this child class.\n",
    "        \n",
    "        :param output_shape (tuple): the shape of the output array.  When this isa single number, it gives the number of output neurons\n",
    "            When this is an array, it gives the dimensions of the array of output neurons.\n",
    "        \"\"\"\n",
    "        Layer.__init__(self, output_shape) # TODO: Pass along any arguments to the parent's initializer here.\n",
    "\n",
    "    def set(self, value):\n",
    "        \"\"\"\n",
    "        TODO: set the `output` of this array to have value `value`.\n",
    "        Raise an error if the size of value is unexpected. An `assert()` is fine for this.\n",
    "        \n",
    "        :param value (numpy array): numpy array of values to represent the input layer (gets converted to a Tensor).\n",
    "        \"\"\"\n",
    "        # assert dimensions are equal\n",
    "        assert value.shape == self.output.shape, f'[!] dimension {value.shape} does not match expected dimension {self.output_shape}'\n",
    "        \n",
    "        self.output = torch.Tensor(value)\n",
    "                \n",
    "    def forward(self):\n",
    "        \"\"\"This layer's values do not change during forward propagation.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, x_layer, W_layer, b_layer):\n",
    "        \"\"\"\n",
    "        TODO: Accept any arguments specific to this child class.\n",
    "        \n",
    "        Raise an error if any of the argument's size do not match as you would expect.\n",
    "        \n",
    "        :param x_layer (Layer): input layer (vector)\n",
    "        :param W_layer (Layer): weights layer (matrix)\n",
    "        :param b_layer (Layer): bias layer (vector)\n",
    "        \"\"\"\n",
    "        Layer.__init__(self, b_layer.output_shape) # TODO: Pass along any arguments to the parent's initializer here.\n",
    "        \n",
    "        assert x_layer.output_shape == W_layer.output_shape[1], f\"[!] dimension mismatch {x_layer.output_shape} & {W_layer.output_shape}\"\n",
    "        assert W_layer.output_shape[0] == b_layer.output_shape, f\"[!] dimension mismatch {W_layer.output_shape} & {b_layer.output_shape}\"\n",
    "        \n",
    "        self.x_layer = x_layer\n",
    "        self.W_layer = W_layer\n",
    "        self.b_layer = b_layer\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        TODO: Set this layer's output based on the outputs of the layers that feed into it.\n",
    "        \"\"\"\n",
    "        self.output = torch.matmul(self.W_layer.output, self.x_layer.output) + self.b_layer.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " output of hidden layer \n",
      " tensor([ 58.8380,  81.9780, 130.9620])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# This example uses your classes to test the output of the entire network.\n",
    "#\n",
    "# You may change this example to match your own implementation if you wish.\n",
    "\n",
    "x_layer = Input(4)\n",
    "x_layer.set(np.array(x))\n",
    "W_layer = Input((3, 4))\n",
    "W_layer.set(np.array(W))\n",
    "b_layer = Input(3)\n",
    "b_layer.set(np.array(b))\n",
    "linear_layer = Linear(x_layer, W_layer, b_layer)\n",
    "\n",
    "linear_layer.forward()\n",
    "print('\\n output of hidden layer \\n', linear_layer.output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes this lab... except for the two **required** parting questions:\n",
    "\n",
    "### <font color=blue>Question 6: Summarize what you learned during this lab.\n",
    "\n",
    "Ans: \n",
    "\n",
    "Throughout this lab, I was able to gain a better understanding of multi-layer perceptrons and the math behind them. In addition, I learned to use PyTorch tensors to represent vectors and matricies to represent the model parameters and input, as well as use core PyTorch functions, like matmul(), to manipulate these. This lab further solidified my understanding of how to interpret the values of the weight matrix between two layers in a neural network. The bigest takeaway from this lab is that the number of rows of the weight matrix tells you how many neurons are in the output layer and the number of columns of the weight matrix tell you the number of neurons in the input layer. This helped me better understand and recognize the shapes of consecutive layers in a multi-layer perceptron. In addition, I learned how inheritance works in Python OOP. Overall, in this lab I was able to manually, as well as utilize PyTorch's library of functions to implement the layers of a feedforward neural network using OOP."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Question 7: Describe what you liked about this lab *or* what could be improved. (Required.)\n",
    "\n",
    "Ans: \n",
    "\n",
    "I enjoyed the format of this lab and how it guides you through the lab. I found this to be very helpful, especially when understanding the shapes of the various layers. It's simple, but I really liked having the diagrams within the notebook itself because it made completing the lab easier and helped me better visualize what the code was doing. One thing that I wish was provided with this lab was more Python OOP explanation because I have not had a lot of experience with this in past courses. Also I think that if the comments on the Layer classes were more descriptive, then this part of the lab would have been less confusing.\n",
    "\n",
    "**NOTE: I wasn't sure when and when not to use the sigmoid activation function, since it was included in the diagram but not mentioned in the report. Therefore, I printed the output vector before and after the activation.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
